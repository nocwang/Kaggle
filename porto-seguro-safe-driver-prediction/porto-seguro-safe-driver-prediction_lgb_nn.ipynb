{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgbm\n",
    "from scipy import sparse as ssp\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def Gini(y_true, y_pred):\n",
    "    # check and get number of samples\n",
    "    assert y_true.shape == y_pred.shape\n",
    "    n_samples = y_true.shape[0]\n",
    "\n",
    "    # sort rows on prediction column\n",
    "    # (from largest to smallest)\n",
    "    arr = np.array([y_true, y_pred]).transpose()\n",
    "    true_order = arr[arr[:, 0].argsort()][::-1, 0]\n",
    "    pred_order = arr[arr[:, 1].argsort()][::-1, 0]\n",
    "\n",
    "    # get Lorenz curves\n",
    "    L_true = np.cumsum(true_order) * 1. / np.sum(true_order)\n",
    "    L_pred = np.cumsum(pred_order) * 1. / np.sum(pred_order)\n",
    "    L_ones = np.linspace(1 / n_samples, 1, n_samples)\n",
    "\n",
    "    # get Gini coefficients (area between curves)\n",
    "    G_true = np.sum(L_ones - L_true)\n",
    "    G_pred = np.sum(L_ones - L_pred)\n",
    "\n",
    "    # normalize to true Gini coefficient\n",
    "    return G_pred * 1. / G_true\n",
    "\n",
    "cv_only = True\n",
    "save_cv = True\n",
    "full_train = False\n",
    "\n",
    "def evalerror(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    return 'gini', Gini(labels, preds), True\n",
    "\n",
    "path = \"../input/\"\n",
    "\n",
    "train = pd.read_csv(path+'train.csv')\n",
    "train_label = train['target']\n",
    "train_id = train['id']\n",
    "test = pd.read_csv(path+'test.csv')\n",
    "test_id = test['id']\n",
    "\n",
    "NFOLDS = 5\n",
    "kfold = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=218)\n",
    "\n",
    "y = train['target'].values\n",
    "drop_feature = [\n",
    "    'id',\n",
    "    'target'\n",
    "]\n",
    "\n",
    "X = train.drop(drop_feature,axis=1)\n",
    "feature_names = X.columns.tolist()\n",
    "cat_features = [c for c in feature_names if ('cat' in c and 'count' not in c)]\n",
    "num_features = [c for c in feature_names if ('cat' not in c and 'calc' not in c)]\n",
    "\n",
    "train['missing'] = (train==-1).sum(axis=1).astype(float)\n",
    "test['missing'] = (test==-1).sum(axis=1).astype(float)\n",
    "num_features.append('missing')\n",
    "\n",
    "for c in cat_features:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(train[c])\n",
    "    train[c] = le.transform(train[c])\n",
    "    test[c] = le.transform(test[c])\n",
    "\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(train[cat_features])\n",
    "X_cat = enc.transform(train[cat_features])\n",
    "X_t_cat = enc.transform(test[cat_features])\n",
    "\n",
    "ind_features = [c for c in feature_names if 'ind' in c]\n",
    "count=0\n",
    "for c in ind_features:\n",
    "    if count==0:\n",
    "        train['new_ind'] = train[c].astype(str)+'_'\n",
    "        test['new_ind'] = test[c].astype(str)+'_'\n",
    "        count+=1\n",
    "    else:\n",
    "        train['new_ind'] += train[c].astype(str)+'_'\n",
    "        test['new_ind'] += test[c].astype(str)+'_'\n",
    "\n",
    "cat_count_features = []\n",
    "for c in cat_features+['new_ind']:\n",
    "    d = pd.concat([train[c],test[c]]).value_counts().to_dict()\n",
    "    train['%s_count'%c] = train[c].apply(lambda x:d.get(x,0))\n",
    "    test['%s_count'%c] = test[c].apply(lambda x:d.get(x,0))\n",
    "    cat_count_features.append('%s_count'%c)\n",
    "\n",
    "train_list = [train[num_features+cat_count_features].values,X_cat,]\n",
    "test_list = [test[num_features+cat_count_features].values,X_t_cat,]\n",
    "\n",
    "X = ssp.hstack(train_list).tocsr()\n",
    "X_test = ssp.hstack(test_list).tocsr()\n",
    "\n",
    "learning_rate = 0.1\n",
    "num_leaves = 15\n",
    "min_data_in_leaf = 2000\n",
    "feature_fraction = 0.6\n",
    "num_boost_round = 10000\n",
    "params = {\"objective\": \"binary\",\n",
    "          \"boosting_type\": \"gbdt\",\n",
    "          \"learning_rate\": learning_rate,\n",
    "          \"num_leaves\": num_leaves,\n",
    "           \"max_bin\": 256,\n",
    "          \"feature_fraction\": feature_fraction,\n",
    "          \"verbosity\": 0,\n",
    "          \"drop_rate\": 0.1,\n",
    "          \"is_unbalance\": False,\n",
    "          \"max_drop\": 50,\n",
    "          \"min_child_samples\": 10,\n",
    "          \"min_child_weight\": 150,\n",
    "          \"min_split_gain\": 0,\n",
    "          \"subsample\": 0.9\n",
    "          }\n",
    "\n",
    "x_score = []\n",
    "final_cv_train = np.zeros(len(train_label))\n",
    "final_cv_pred = np.zeros(len(test_id))\n",
    "for s in xrange(16):\n",
    "    cv_train = np.zeros(len(train_label))\n",
    "    cv_pred = np.zeros(len(test_id))\n",
    "\n",
    "    params['seed'] = s\n",
    "\n",
    "    if cv_only:\n",
    "        kf = kfold.split(X, train_label)\n",
    "\n",
    "        best_trees = []\n",
    "        fold_scores = []\n",
    "\n",
    "        for i, (train_fold, validate) in enumerate(kf):\n",
    "            X_train, X_validate, label_train, label_validate = \\\n",
    "                X[train_fold, :], X[validate, :], train_label[train_fold], train_label[validate]\n",
    "            dtrain = lgbm.Dataset(X_train, label_train)\n",
    "            dvalid = lgbm.Dataset(X_validate, label_validate, reference=dtrain)\n",
    "            bst = lgbm.train(params, dtrain, num_boost_round, valid_sets=dvalid, feval=evalerror, verbose_eval=100,\n",
    "                            early_stopping_rounds=100)\n",
    "            best_trees.append(bst.best_iteration)\n",
    "            cv_pred += bst.predict(X_test, num_iteration=bst.best_iteration)\n",
    "            cv_train[validate] += bst.predict(X_validate)\n",
    "\n",
    "            score = Gini(label_validate, cv_train[validate])\n",
    "            print(score)\n",
    "            fold_scores.append(score)\n",
    "\n",
    "        cv_pred /= NFOLDS\n",
    "        final_cv_train += cv_train\n",
    "        final_cv_pred += cv_pred\n",
    "\n",
    "        print(\"cv score:\")\n",
    "        print Gini(train_label, cv_train)\n",
    "        print \"current score:\", Gini(train_label, final_cv_train / (s + 1.)), s+1\n",
    "        print(fold_scores)\n",
    "        print(best_trees, np.mean(best_trees))\n",
    "\n",
    "        x_score.append(Gini(train_label, cv_train))\n",
    "\n",
    "print(x_score)\n",
    "pd.DataFrame({'id': test_id, 'target': final_cv_pred / 16.}).to_csv('../model/lgbm3_pred_avg.csv', index=False)\n",
    "pd.DataFrame({'id': train_id, 'target': final_cv_train / 16.}).to_csv('../model/lgbm3_cv_avg.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN+keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout, Embedding, Flatten, Input, merge\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from time import time\n",
    "import datetime\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from util import Gini, interaction_features\n",
    "from itertools import combinations\n",
    "from util import proj_num_on_cat\n",
    "from scipy import sparse\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "cv_only = True\n",
    "save_cv = True\n",
    "\n",
    "NFOLDS = 5\n",
    "kfold = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=218)\n",
    "\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "train_label = train['target']\n",
    "train_id = train['id']\n",
    "del train['target'], train['id']\n",
    "\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "test_id = test['id']\n",
    "del test['id']\n",
    "\n",
    "cat_fea = [x for x in list(train) if 'cat' in x]\n",
    "bin_fea = [x for x in list(train) if 'bin' in x]\n",
    "\n",
    "train['missing'] = (train==-1).sum(axis=1).astype(float)\n",
    "test['missing'] = (test==-1).sum(axis=1).astype(float)\n",
    "\n",
    "# include interactions\n",
    "for e, (x, y) in enumerate(combinations(['ps_car_13', 'ps_ind_03', 'ps_reg_03', 'ps_ind_15', 'ps_reg_01', 'ps_ind_01'], 2)):\n",
    "    train, test = interaction_features(train, test, x, y, e)\n",
    "\n",
    "num_features = [c for c in list(train) if ('cat' not in c and 'calc' not in c)]\n",
    "num_features.append('missing')\n",
    "inter_fea = [x for x in list(train) if 'inter' in x]\n",
    "\n",
    "feature_names = list(train)\n",
    "ind_features = [c for c in feature_names if 'ind' in c]\n",
    "count = 0\n",
    "for c in ind_features:\n",
    "    if count == 0:\n",
    "        train['new_ind'] = train[c].astype(str)\n",
    "        count += 1\n",
    "    else:\n",
    "        train['new_ind'] += '_' + train[c].astype(str)\n",
    "\n",
    "ind_features = [c for c in feature_names if 'ind' in c]\n",
    "count = 0\n",
    "for c in ind_features:\n",
    "    if count == 0:\n",
    "        test['new_ind'] = test[c].astype(str)\n",
    "        count += 1\n",
    "    else:\n",
    "        test['new_ind'] += '_' + test[c].astype(str)\n",
    "\n",
    "reg_features = [c for c in feature_names if 'reg' in c]\n",
    "count = 0\n",
    "for c in reg_features:\n",
    "    if count == 0:\n",
    "        train['new_reg'] = train[c].astype(str)\n",
    "        count += 1\n",
    "    else:\n",
    "        train['new_reg'] += '_' + train[c].astype(str)\n",
    "\n",
    "reg_features = [c for c in feature_names if 'reg' in c]\n",
    "count = 0\n",
    "for c in reg_features:\n",
    "    if count == 0:\n",
    "        test['new_reg'] = test[c].astype(str)\n",
    "        count += 1\n",
    "    else:\n",
    "        test['new_reg'] += '_' + test[c].astype(str)\n",
    "\n",
    "car_features = [c for c in feature_names if 'car' in c]\n",
    "count = 0\n",
    "for c in car_features:\n",
    "    if count == 0:\n",
    "        train['new_car'] = train[c].astype(str)\n",
    "        count += 1\n",
    "    else:\n",
    "        train['new_car'] += '_' + train[c].astype(str)\n",
    "\n",
    "car_features = [c for c in feature_names if 'car' in c]\n",
    "count = 0\n",
    "for c in car_features:\n",
    "    if count == 0:\n",
    "        test['new_car'] = test[c].astype(str)\n",
    "        count += 1\n",
    "    else:\n",
    "        test['new_car'] += '_' + test[c].astype(str)\n",
    "\n",
    "train_cat = train[cat_fea]\n",
    "train_num = train[[x for x in list(train) if x in num_features]]\n",
    "test_cat = test[cat_fea]\n",
    "test_num = test[[x for x in list(train) if x in num_features]]\n",
    "\n",
    "max_cat_values = []\n",
    "for c in cat_fea:\n",
    "    le = LabelEncoder()\n",
    "    x = le.fit_transform(pd.concat([train_cat, test_cat])[c])\n",
    "    train_cat[c] = le.transform(train_cat[c])\n",
    "    test_cat[c] = le.transform(test_cat[c])\n",
    "    max_cat_values.append(np.max(x))\n",
    "\n",
    "# xgboost prediction\n",
    "train_fea0, test_fea0 = pickle.load(open(\"../input/fea0.pk\"))\n",
    "\n",
    "cat_count_features = []\n",
    "for c in cat_fea + ['new_ind','new_reg','new_car']:\n",
    "    d = pd.concat([train[c],test[c]]).value_counts().to_dict()\n",
    "    train['%s_count'%c] = train[c].apply(lambda x:d.get(x,0))\n",
    "    test['%s_count'%c] = test[c].apply(lambda x:d.get(x,0))\n",
    "    cat_count_features.append('%s_count'%c)\n",
    "\n",
    "\n",
    "print(train_num.dtypes)\n",
    "train_list = [train_num.replace([np.inf, -np.inf, np.nan], 0), train[cat_count_features], train_fea0]\n",
    "test_list = [test_num.replace([np.inf, -np.inf, np.nan], 0), test[cat_count_features], test_fea0]\n",
    "\n",
    "#feature aggregation\n",
    "for t in ['ps_car_13', 'ps_ind_03', 'ps_reg_03', 'ps_ind_15', 'ps_reg_01', 'ps_ind_01']:\n",
    "    for g in ['ps_car_13', 'ps_ind_03', 'ps_reg_03', 'ps_ind_15', 'ps_reg_01', 'ps_ind_01', 'ps_ind_05_cat']:\n",
    "        if t != g:\n",
    "            s_train, s_test = proj_num_on_cat(train, test, target_column=t, group_column=g)\n",
    "            train_list.append(s_train)\n",
    "            test_list.append(s_test)\n",
    "X = sparse.hstack(train_list).tocsr()\n",
    "X_test = sparse.hstack(test_list).tocsr()\n",
    "\n",
    "all_data = np.vstack([X.toarray(), X_test.toarray()])\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(all_data)\n",
    "X = scaler.transform(X.toarray())\n",
    "X_test = scaler.transform(X_test.toarray())\n",
    "print(X.shape, X_test.shape)\n",
    "\n",
    "\n",
    "cv_train = np.zeros(len(train_label))\n",
    "cv_pred = np.zeros(len(test_id))\n",
    "\n",
    "X_cat = train_cat.as_matrix()\n",
    "X_test_cat = test_cat.as_matrix()\n",
    "\n",
    "x_test_cat = []\n",
    "for i in xrange(X_test_cat.shape[1]):\n",
    "    x_test_cat.append(X_test_cat[:, i].reshape(-1, 1))\n",
    "x_test_cat.append(X_test)\n",
    "\n",
    "def nn_model():\n",
    "    inputs = []\n",
    "    flatten_layers = []\n",
    "    for e, c in enumerate(cat_fea):\n",
    "        input_c = Input(shape=(1, ), dtype='int32')\n",
    "        num_c = max_cat_values[e]\n",
    "        embed_c = Embedding(\n",
    "            num_c,\n",
    "            6,\n",
    "            input_length=1\n",
    "        )(input_c)\n",
    "        embed_c = Dropout(0.25)(embed_c)\n",
    "        flatten_c = Flatten()(embed_c)\n",
    "\n",
    "        inputs.append(input_c)\n",
    "        flatten_layers.append(flatten_c)\n",
    "\n",
    "    input_num = Input(shape=(X.shape[1],), dtype='float32')\n",
    "    flatten_layers.append(input_num)\n",
    "    inputs.append(input_num)\n",
    "\n",
    "    flatten = merge(flatten_layers, mode='concat')\n",
    "\n",
    "    fc1 = Dense(512, init='he_normal')(flatten)\n",
    "    fc1 = PReLU()(fc1)\n",
    "    fc1 = BatchNormalization()(fc1)\n",
    "    fc1 = Dropout(0.75)(fc1)\n",
    "\n",
    "    fc1 = Dense(64, init='he_normal')(fc1)\n",
    "    fc1 = PReLU()(fc1)\n",
    "    fc1 = BatchNormalization()(fc1)\n",
    "    fc1 = Dropout(0.5)(fc1)\n",
    "\n",
    "    outputs = Dense(1, init='he_normal', activation='sigmoid')(fc1)\n",
    "\n",
    "    model = Model(input = inputs, output = outputs)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    return (model)\n",
    "\n",
    "num_seeds = 5\n",
    "begintime = time()\n",
    "if cv_only:\n",
    "    for s in xrange(num_seeds):\n",
    "        np.random.seed(s)\n",
    "        for (inTr, inTe) in kfold.split(X, train_label):\n",
    "            xtr = X[inTr]\n",
    "            ytr = train_label[inTr]\n",
    "            xte = X[inTe]\n",
    "            yte = train_label[inTe]\n",
    "\n",
    "            xtr_cat = X_cat[inTr]\n",
    "            xte_cat = X_cat[inTe]\n",
    "\n",
    "            # get xtr xte cat\n",
    "            xtr_cat_list, xte_cat_list = [], []\n",
    "            for i in xrange(xtr_cat.shape[1]):\n",
    "                xtr_cat_list.append(xtr_cat[:, i].reshape(-1, 1))\n",
    "                xte_cat_list.append(xte_cat[:, i].reshape(-1, 1))\n",
    "\n",
    "            xtr_cat_list.append(xtr)\n",
    "            xte_cat_list.append(xte)\n",
    "\n",
    "            model = nn_model()\n",
    "            def get_rank(x):\n",
    "                return pd.Series(x).rank(pct=True).values\n",
    "            model.fit(xtr_cat_list, ytr, epochs=20, batch_size=512, verbose=2, validation_data=[xte_cat_list, yte])\n",
    "            cv_train[inTe] += get_rank(model.predict(x=xte_cat_list, batch_size=512, verbose=0)[:, 0])\n",
    "            print(Gini(train_label[inTe], cv_train[inTe]))\n",
    "            cv_pred += get_rank(model.predict(x=x_test_cat, batch_size=512, verbose=0)[:, 0])\n",
    "        print(s)\n",
    "        print(Gini(train_label, cv_train / (1. * (s + 1))))\n",
    "        print(str(datetime.timedelta(seconds=time() - begintime)))\n",
    "    if save_cv:\n",
    "        pd.DataFrame({'id': test_id, 'target': get_rank(cv_pred * 1./ (NFOLDS * num_seeds))}).to_csv('../model/keras5_pred.csv', index=False)\n",
    "pd.DataFrame({'id': train_id, 'target': get_rank(cv_train * 1. / num_seeds)}).to_csv('../model/keras5_cv.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
